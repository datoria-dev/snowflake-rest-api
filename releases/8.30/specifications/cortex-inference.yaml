openapi: 3.0.2
info:
  title: Cortex Inference API
  description: OpenAPI 3.0 specification for the Cortex REST API
  version: 0.1.0
  contact:
    name: Snowflake, Inc.
    url: https://snowflake.com
    email: support@snowflake.com
paths:
  /api/v2/cortex/inference:complete:
    post:
      summary: Perform LLM text completion inference.
      tags:
      - cortex-inference
      description: Perform LLM text completion inference, similar to snowflake.cortex.Complete.
      operationId: cortexLLMInferenceComplete
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CompleteRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/NonStreamingCompleteResponse'
            text/event-stream:
              schema:
                $ref: '#/components/schemas/StreamingCompleteResponse'
        '400':
          $ref: common.yaml#/components/responses/400BadRequest
        '401':
          $ref: common.yaml#/components/responses/401Unauthorized
        '403':
          $ref: common.yaml#/components/responses/403Forbidden
        '404':
          $ref: common.yaml#/components/responses/404NotFound
        '405':
          $ref: common.yaml#/components/responses/405MethodNotAllowed
        '500':
          $ref: common.yaml#/components/responses/500InternalServerError
        '503':
          $ref: common.yaml#/components/responses/503ServiceUnavailable
        '504':
          $ref: common.yaml#/components/responses/504GatewayTimeout
components:
  schemas:
    CompleteRequest:
      type: object
      description: LLM text completion request.
      properties:
        model:
          description: The model name. See documentation for possible values.
          type: string
        messages:
          type: array
          items:
            type: object
            properties:
              content:
                type: string
                description: The text completion prompt, e.g. 'What is a Large Language
                  Model?'.
            required:
            - content
          minItems: 1
          maxItems: 1
        stream:
          description: If true, the response content is streamed as JSON-structured
            server-sent events. If false, a single JSON response is returned.
          type: boolean
          default: false
        temperature:
          description: Temperature controls the amount of randomness used in response
            generation. A higher temperature corresponds to more randomness.
          type: number
          default: 0
          minimum: 0.0
        top_p:
          description: Threshold probability for nucleus sampling. A higher top-p
            value increases the diversity of tokens that the model considers, while
            a lower value results in more predictable output.
          type: number
          default: 1.0
          minimum: 0.0
          maximum: 1.0
        max_output_tokens:
          description: The maximum number of output tokens to produce. The default
            value is model-dependent.
          type: integer
          default: 4096
          minimum: 0
      required:
      - model
      - messages
    NonStreamingCompleteResponse:
      type: object
      description: Text-completion response for non-streaming request.
      properties:
        choices:
          type: array
          items:
            type: object
            properties:
              message:
                type: object
                properties:
                  content:
                    type: string
                    description: The text completion response.
        usage:
          type: object
          title: Usage
          properties:
            prompt_tokens:
              type: integer
              description: Input token count.
            completion_tokens:
              type: integer
              description: Output token count.
            total_tokens:
              type: integer
              description: Sum of prompt_tokens and completion_tokens.
      required:
      - texts
    StreamingCompleteResponse:
      type: object
      description: Server-sent events for streaming text-completion updates.
      x-events:
        data:
          $ref: '#/components/schemas/StreamingCompleteResponseDataEvent'
    StreamingCompleteResponseDataEvent:
      type: object
      description: Streaming text-completion response event.
      properties:
        choices:
          type: array
          items:
            type: object
            properties:
              delta:
                type: object
                properties:
                  content:
                    type: string
                    description: Change in text-completion response content since
                      previous event.
tags:
- name: cortex-inference
