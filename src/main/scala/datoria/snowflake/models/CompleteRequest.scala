/** merged spec
  * merged spec
  *
  * The version of the OpenAPI document: 1.0.0
  * Contact: team@openapitools.org
  *
  * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
  * https://openapi-generator.tech
  * Do not edit the class manually.
  */
package datoria.snowflake.models

import io.circe.*
import io.circe.syntax.*
import io.circe.{Decoder, Encoder}

import scala.collection.immutable.Seq

/** LLM text completion request.
  * @param model The model name. See documentation for possible values.
  * @param messages 
  * @param temperature Temperature controls the amount of randomness used in response generation. A higher temperature corresponds to more randomness.
  * @param topP Threshold probability for nucleus sampling. A higher top-p value increases the diversity of tokens that the model considers, while a lower value results in more predictable output.
  * @param maxTokens The maximum number of output tokens to produce. The default value is model-dependent.
  * @param guardrails 
  */
case class CompleteRequest(
    model: String,
    messages: Seq[CompleteRequestMessagesInner],
    temperature: Option[BigDecimal] = None,
    topP: Option[BigDecimal] = None,
    maxTokens: Option[Int] = None,
    guardrails: Option[GuardrailsConfig] = None
)
  
object CompleteRequest {
  given encoderCompleteRequest: Encoder[CompleteRequest] = Encoder.instance { t =>
    Json.fromFields{
      Seq(
        Some("model" -> t.model.asJson),
        Some("messages" -> t.messages.asJson),
        t.temperature.map(v => "temperature" -> v.asJson),
        t.topP.map(v => "top_p" -> v.asJson),
        t.maxTokens.map(v => "max_tokens" -> v.asJson),
        t.guardrails.map(v => "guardrails" -> v.asJson)
      ).flatten
    }
  }
  given decoderCompleteRequest: Decoder[CompleteRequest] = Decoder.instance { c =>
    for {
      model <- c.downField("model").as[String]
      messages <- c.downField("messages").as[Seq[CompleteRequestMessagesInner]]
      temperature <- c.downField("temperature").as[Option[BigDecimal]]
      topP <- c.downField("top_p").as[Option[BigDecimal]]
      maxTokens <- c.downField("max_tokens").as[Option[Int]]
      guardrails <- c.downField("guardrails").as[Option[GuardrailsConfig]]
    } yield CompleteRequest(
      model = model,
      messages = messages,
      temperature = temperature,
      topP = topP,
      maxTokens = maxTokens,
      guardrails = guardrails
    )
  }
}

