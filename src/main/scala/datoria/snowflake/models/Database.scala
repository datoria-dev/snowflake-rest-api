/** merged spec
  * merged spec
  *
  * The version of the OpenAPI document: 1.0.0
  * Contact: team@openapitools.org
  *
  * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
  * https://openapi-generator.tech
  * Do not edit the class manually.
  */
package datoria.snowflake.models

import io.circe.*
import io.circe.syntax.*
import io.circe.{Decoder, Encoder}

import java.time.Instant

/** Snowflake database object.
  * @param createdOn Date and time the database was created.
  * @param name A Snowflake object identifier.
  * @param kind 
  * @param isDefault Whether the database is the default database for a user.
  * @param isCurrent Current database for the session.
  * @param origin 
  * @param owner Name of the role that owns the database.
  * @param comment Optional comment in which to store information related to the database.
  * @param options 
  * @param retentionTime Number of days that historical data is retained for Time Travel.
  * @param droppedOn Date and time the database was dropped.
  * @param budget Budget that defines a monthly spending limit on the compute costs for a Snowflake account or a custom group of Snowflake objects.
  * @param ownerRoleType Type of role that owns the object, either ROLE or DATABASE_ROLE
  * @param dataRetentionTimeInDays Specifies the number of days for which Time Travel actions (CLONE and UNDROP) can be performed on the database, as well as specifying the default Time Travel retention time for all schemas created in the database.
  * @param defaultDdlCollation Default collation specification for all schemas and tables added to the database. You an override the default at the schema and individual table levels.
  * @param logLevel Severity level of messages that should be ingested and made available in the active event table. Currently, Snowflake supports only `TRACE`, `DEBUG`, `INFO`, `WARN`, `ERROR`, `FATAL` and `OFF`.
  * @param maxDataExtensionTimeInDays Maximum number of days for which Snowflake can extend the data retention period for tables in the database to prevent streams on the tables from becoming stale.
  * @param suspendTaskAfterNumFailures Maximum number of consecutive failed task runs before the current task is suspended automatically.
  * @param traceLevel How trace events are ingested into the event table. Currently, Snowflake supports only `ALWAYS`, `ON_EVENT`, and `OFF`.
  * @param userTaskManagedInitialWarehouseSize Size of the compute resources to provision for the first run of the task, before a task history is available for Snowflake to determine an ideal size.
  * @param userTaskTimeoutMs Time limit, in milliseconds, for a single run of the task before it times out.
  */
case class Database(
    createdOn: Option[Instant] = None,
    name: String,
    kind: Option[DatabaseKind] = None,
    isDefault: Option[Boolean] = None,
    isCurrent: Option[Boolean] = None,
    origin: Option[String] = None,
    owner: Option[String] = None,
    comment: Option[String] = None,
    options: Option[String] = None,
    retentionTime: Option[Int] = None,
    droppedOn: Option[Instant] = None,
    budget: Option[String] = None,
    ownerRoleType: Option[String] = None,
    dataRetentionTimeInDays: Option[Int] = None,
    defaultDdlCollation: Option[String] = None,
    logLevel: Option[String] = None,
    maxDataExtensionTimeInDays: Option[Int] = None,
    suspendTaskAfterNumFailures: Option[Int] = None,
    traceLevel: Option[String] = None,
    userTaskManagedInitialWarehouseSize: Option[String] = None,
    userTaskTimeoutMs: Option[Int] = None
)
  
object Database {
  given encoderDatabase: Encoder[Database] = Encoder.instance { t =>
    Json.fromFields{
      Seq(
        t.createdOn.map(v => "created_on" -> v.asJson),
        Some("name" -> t.name.asJson),
        t.kind.map(v => "kind" -> v.asJson),
        t.isDefault.map(v => "is_default" -> v.asJson),
        t.isCurrent.map(v => "is_current" -> v.asJson),
        t.origin.map(v => "origin" -> v.asJson),
        t.owner.map(v => "owner" -> v.asJson),
        t.comment.map(v => "comment" -> v.asJson),
        t.options.map(v => "options" -> v.asJson),
        t.retentionTime.map(v => "retention_time" -> v.asJson),
        t.droppedOn.map(v => "dropped_on" -> v.asJson),
        t.budget.map(v => "budget" -> v.asJson),
        t.ownerRoleType.map(v => "owner_role_type" -> v.asJson),
        t.dataRetentionTimeInDays.map(v => "data_retention_time_in_days" -> v.asJson),
        t.defaultDdlCollation.map(v => "default_ddl_collation" -> v.asJson),
        t.logLevel.map(v => "log_level" -> v.asJson),
        t.maxDataExtensionTimeInDays.map(v => "max_data_extension_time_in_days" -> v.asJson),
        t.suspendTaskAfterNumFailures.map(v => "suspend_task_after_num_failures" -> v.asJson),
        t.traceLevel.map(v => "trace_level" -> v.asJson),
        t.userTaskManagedInitialWarehouseSize.map(v => "user_task_managed_initial_warehouse_size" -> v.asJson),
        t.userTaskTimeoutMs.map(v => "user_task_timeout_ms" -> v.asJson)
      ).flatten
    }
  }
  given decoderDatabase: Decoder[Database] = Decoder.instance { c =>
    for {
      createdOn <- c.downField("created_on").as[Option[Instant]]
      name <- c.downField("name").as[String]
      kind <- c.downField("kind").as[Option[DatabaseKind]]
      isDefault <- c.downField("is_default").as[Option[Boolean]]
      isCurrent <- c.downField("is_current").as[Option[Boolean]]
      origin <- c.downField("origin").as[Option[String]]
      owner <- c.downField("owner").as[Option[String]]
      comment <- c.downField("comment").as[Option[String]]
      options <- c.downField("options").as[Option[String]]
      retentionTime <- c.downField("retention_time").as[Option[Int]]
      droppedOn <- c.downField("dropped_on").as[Option[Instant]]
      budget <- c.downField("budget").as[Option[String]]
      ownerRoleType <- c.downField("owner_role_type").as[Option[String]]
      dataRetentionTimeInDays <- c.downField("data_retention_time_in_days").as[Option[Int]]
      defaultDdlCollation <- c.downField("default_ddl_collation").as[Option[String]]
      logLevel <- c.downField("log_level").as[Option[String]]
      maxDataExtensionTimeInDays <- c.downField("max_data_extension_time_in_days").as[Option[Int]]
      suspendTaskAfterNumFailures <- c.downField("suspend_task_after_num_failures").as[Option[Int]]
      traceLevel <- c.downField("trace_level").as[Option[String]]
      userTaskManagedInitialWarehouseSize <- c.downField("user_task_managed_initial_warehouse_size").as[Option[String]]
      userTaskTimeoutMs <- c.downField("user_task_timeout_ms").as[Option[Int]]
    } yield Database(
      createdOn = createdOn,
      name = name,
      kind = kind,
      isDefault = isDefault,
      isCurrent = isCurrent,
      origin = origin,
      owner = owner,
      comment = comment,
      options = options,
      retentionTime = retentionTime,
      droppedOn = droppedOn,
      budget = budget,
      ownerRoleType = ownerRoleType,
      dataRetentionTimeInDays = dataRetentionTimeInDays,
      defaultDdlCollation = defaultDdlCollation,
      logLevel = logLevel,
      maxDataExtensionTimeInDays = maxDataExtensionTimeInDays,
      suspendTaskAfterNumFailures = suspendTaskAfterNumFailures,
      traceLevel = traceLevel,
      userTaskManagedInitialWarehouseSize = userTaskManagedInitialWarehouseSize,
      userTaskTimeoutMs = userTaskTimeoutMs
    )
  }
}

