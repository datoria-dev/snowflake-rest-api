openapi: 3.0.2
info:
  title: Cortex Inference API
  description: OpenAPI 3.0 specification for the Cortex REST API
  version: 0.1.0
  contact:
    name: Snowflake, Inc.
    url: https://snowflake.com
    email: support@snowflake.com
paths:
  /api/v2/cortex/inference:complete:
    post:
      summary: Perform LLM text completion inference.
      tags:
      - cortex-inference
      description: Perform LLM text completion inference, similar to snowflake.cortex.Complete.
      operationId: cortexLLMInferenceComplete
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CompleteRequest'
      responses:
        '200':
          description: OK
          content:
            text/event-stream:
              schema:
                $ref: '#/components/schemas/StreamingCompleteResponse'
        '400':
          $ref: common.yaml#/components/responses/400BadRequest
        '401':
          $ref: common.yaml#/components/responses/401Unauthorized
        '403':
          $ref: common.yaml#/components/responses/403Forbidden
        '404':
          $ref: common.yaml#/components/responses/404NotFound
        '405':
          $ref: common.yaml#/components/responses/405MethodNotAllowed
        '500':
          $ref: common.yaml#/components/responses/500InternalServerError
        '503':
          $ref: common.yaml#/components/responses/503ServiceUnavailable
        '504':
          $ref: common.yaml#/components/responses/504GatewayTimeout
components:
  schemas:
    CompleteRequest:
      type: object
      description: LLM text completion request.
      properties:
        model:
          description: The model name. See documentation for possible values.
          type: string
        messages:
          type: array
          items:
            type: object
            properties:
              content:
                type: string
                description: The text completion prompt, e.g. 'What is a Large Language
                  Model?'.
            required:
            - content
          minItems: 1
          maxItems: 1
        temperature:
          description: Temperature controls the amount of randomness used in response
            generation. A higher temperature corresponds to more randomness.
          type: number
          default: 0
          minimum: 0.0
        top_p:
          description: Threshold probability for nucleus sampling. A higher top-p
            value increases the diversity of tokens that the model considers, while
            a lower value results in more predictable output.
          type: number
          default: 1.0
          minimum: 0.0
          maximum: 1.0
        max_tokens:
          description: The maximum number of output tokens to produce. The default
            value is model-dependent.
          type: integer
          default: 4096
          minimum: 0
      required:
      - model
      - messages
    StreamingCompleteResponse:
      type: object
      description: Server-sent events for streaming text-completion updates.
      x-events:
        data:
          $ref: '#/components/schemas/StreamingCompleteResponseDataEvent'
    StreamingCompleteResponseDataEvent:
      type: object
      description: Streaming text-completion response event.
      properties:
        choices:
          type: array
          items:
            type: object
            properties:
              delta:
                type: object
                properties:
                  content:
                    type: string
                    description: Change in text-completion response content since
                      previous event.
  securitySchemes:
    KeyPair:
      $ref: common.yaml#/components/securitySchemes/KeyPair
    ExternalOAuth:
      $ref: common.yaml#/components/securitySchemes/ExternalOAuth
    SnowflakeOAuth:
      $ref: common.yaml#/components/securitySchemes/SnowflakeOAuth
security:
- KeyPair: []
- ExternalOAuth: []
- SnowflakeOAuth: []
